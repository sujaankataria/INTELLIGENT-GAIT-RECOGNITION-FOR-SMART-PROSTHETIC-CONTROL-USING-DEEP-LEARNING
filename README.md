# ğŸ¦¿ Intelligent Gait Recognition for Smart Prosthetics using Deep Learning

## ğŸ“˜ Overview
This project presents a unified deep learning framework for **human gait recognition** using the **HuGaDB v2 (Human Gait Database)** wearable IMU dataset.  
It aims to enable **adaptive prosthetic control** by accurately classifying lower-limb activities through sensor-based motion data.

A series of experiments (EXP-001 through EXP-009) were conducted to compare major time-series architectures including **LSTM**, **BiLSTM**, **GRU**, **CNN-LSTM**, **TCN**, **Transformer**, and **ResNet1D**, using a consistent preprocessing and training pipeline.

---

## ğŸ§© System Architecture

```text
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Wearable IMU Sensors    â”‚
 â”‚ (Foot, Shank, Thigh)    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Preprocessing &         â”‚
 â”‚ Segmentation            â”‚
 â”‚ (2s windows, 50% overlap,
 â”‚  z-score normalization) â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Deep Learning Models    â”‚
 â”‚ (LSTM / GRU / TCN / etc.)â”‚
 â”‚ Activity Classification â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Prosthetic Control      â”‚
 â”‚ (Adaptive Actuation)    â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Motivation
Current prosthetic systems rely on fixed rule-based control, leading to latency and poor adaptation across users.  
This project explores **sensor-driven deep learning** for real-time gait recognition, providing the basis for an intelligent control module capable of adjusting to user intent and walking dynamics.

---

## ğŸ“‚ Repository Structure
```
project_root/
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ config_exp001.yaml
â”‚   â”œâ”€â”€ config_exp002.yaml
â”‚   â””â”€â”€ ... config_exp009.yaml
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ HuGaDB/              # raw and preprocessed sensor data
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ lstm.py
â”‚   â”œâ”€â”€ bilstm.py
â”‚   â”œâ”€â”€ gru.py
â”‚   â”œâ”€â”€ cnn_lstm.py
â”‚   â”œâ”€â”€ tcn.py
â”‚   â”œâ”€â”€ transformer.py
â”‚   â””â”€â”€ resnet1d.py
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â””â”€â”€ model_factory.py
â”‚
â”œâ”€â”€ train.py
â”œâ”€â”€ evaluate.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ references.bib
```

---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Environment Setup
```bash
conda create -n gait python=3.10
conda activate gait
pip install -r requirements.txt
```

### 2ï¸âƒ£ Dataset
Download **HuGaDB v2** dataset from Kaggle:  
ğŸ‘‰ [https://www.kaggle.com/datasets/romanchereshnev/hugadb-human-gait-database](https://www.kaggle.com/datasets/romanchereshnev/hugadb-human-gait-database)

Place the extracted folder under:
```
data/HuGaDB/
```

### 3ï¸âƒ£ Running an Experiment
```bash
python train.py --config configs/config_exp003.yaml
```

Trained models, logs, and plots will be stored automatically in:
```
artifacts/exp003/
```

---

## ğŸ§ª Experiments Summary

| Experiment | Model Type  | Key Features | Accuracy | Macro F1 | Notes |
|-------------|-------------|--------------|-----------|-----------|-------|
| EXP-001 | LSTM | 2 layers, 128 hidden | 0.861 | 0.792 | Baseline recurrent model |
| EXP-002 | BiLSTM | Bidirectional variant | 0.866 | 0.801 | Slight improvement over LSTM |
| EXP-005 | GRU | Gated recurrent unit | **0.874** | **0.809** | Best overall performer |
| EXP-007 | TCN | Temporal convolutional | 0.854 | 0.788 | Efficient but less stable |
| EXP-009 | Transformer | Self-attention model | 0.847 | 0.781 | High variance, overfit risk |

---

## ğŸ“Š Evaluation Metrics
- **Accuracy**
- **Precision / Recall / F1 (macro & weighted)**
- **Confusion Matrix**
- **Training vs Validation Loss Curves**
- **Parameter Count & Inference Time**

---

## ğŸ”¬ Key Findings
- Recurrent architectures (GRU/LSTM) perform comparably or better than attention-based ones under resource constraints.  
- GRU achieves high accuracy with fewer parameters â†’ ideal for **embedded prosthetic hardware**.  
- Temporal convolutional models (TCN) offer robust parallelism but require fine-tuning of dilation factors.  
- Transformers benefit from longer input windows but tend to overfit smaller datasets.

---

## ğŸš€ Future Work
- Integrate the best-performing model (GRU) into a **closed-loop prosthetic controller**.  
- Perform **real-time inference testing** on embedded boards (e.g., NVIDIA Jetson Nano).  
- Explore **transfer learning** across subjects and sensor setups.  
- Extend to **multimodal fusion** with EMG and pressure sensors.

---

## ğŸ§¾ Citation
If you use this repository or its experimental framework, please cite:

```
@inproceedings{kataria2025gait,
  title={Intelligent Gait Recognition for Smart Prosthetics using Deep Learning},
  author={Kataria, Sujaan and Basu, Shatabdi},
  booktitle={Proceedings of the IEEE International Conference on Biomedical Systems},
  year={2025}
}
```

---

## ğŸ§‘â€ğŸ’» Authors
**Sujaan Kataria** â€“ B.Tech CSE (Data Science), Manipal University Jaipur  
**Dr. Shatabdi Basu** â€“ Associate Professor, Department of CSE, MUJ  

---

## ğŸ“š References
See full list in [`references.bib`](references.bib).

---

## ğŸ§© License
This project is released under the **MIT License**.  
You are free to use, modify, and distribute it for academic or research purposes with appropriate attribution.

---

> **Last Updated:** October 2025  
> **Repository Maintainer:** Sujaan Kataria (sujaan.kataria@learner.muj.edu.in)
